import os
import cv2
import joblib
import numpy as np
import mediapipe as mp
import onnxruntime as ort
from tools import augmentate
from sklearn.svm import LinearSVC
from database import session, Embedding

model_dir = os.path.join(os.path.dirname(__file__), 'models')
rec_path = os.path.join(model_dir, 'quantized_model.onnx')
cls_path = os.path.join(model_dir, 'face_classifier.pkl')
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    static_image_mode=False,  # False ƒë·ªÉ s·ª≠ d·ª•ng video (dynamic)
    max_num_faces=1,  # S·ªë l∆∞·ª£ng khu√¥n m·∫∑t t·ªëi ƒëa c√≥ th·ªÉ ph√°t hi·ªán trong m·ªói khung h√¨nh
    refine_landmarks=True,  # L·ªçc landmarks (ƒëi·ªÉm m·ªëc) ƒë·ªÉ c√≥ ƒë·ªô ch√≠nh x√°c cao h∆°n
    min_detection_confidence=0.5,  # M·ª©c ƒë·ªô tin c·∫≠y t·ªëi thi·ªÉu ƒë·ªÉ ph√°t hi·ªán khu√¥n m·∫∑t
)

def load_model():
    #Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt
    if not os.path.exists('models'):
        os.makedirs('models')

    #Ki·ªÉm tra File ONNX
    rec_model = ort.InferenceSession(rec_path)
    print("Model ONNX s·∫µn s√†ng")

    #Ki·ªÉm tra File SVC
    if not os.path.exists(cls_path):
        print("File SVC ch∆∞a t·ªìn t·∫°i, ƒëang t√°i t·∫°o...")
        cls_model = None
    else:
        cls_model = joblib.load(cls_path)
        print("Model SVC s·∫µn s√†ng")
    return rec_model, cls_model

rec_model, cls_model = load_model()
cls_ready = True
if cls_model == None:
    cls_ready = False
print(cls_ready)


def detect_face(image):
    """
    Ph√°t hi·ªán khu√¥n m·∫∑t trong ·∫£nh v√† tr√≠ch xu·∫•t v√πng m·∫∑t.
    - Tham s·ªë: ·∫£nh g·ªëc
    - Tr·∫£ v·ªÅ: ·∫£nh khu√¥n m·∫∑t ƒë√£ c·∫Øt v√† t·ªça ƒë·ªô bounding box
    """
    global mp_face_mesh, face_mesh
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = face_mesh.process(image_rgb)
    if results.multi_face_landmarks and len(results.multi_face_landmarks) > 0:
        face_landmarks = results.multi_face_landmarks[0].landmark
        
        # L·∫•y c√°c ƒëi·ªÉm landmark c·ªßa m·∫Øt tr√°i v√† m·∫Øt ph·∫£i
        left_eye = [face_landmarks[i] for i in range(33, 133)]  # Landmark c·ªßa m·∫Øt tr√°i (index c√≥ th·ªÉ thay ƒë·ªïi)
        right_eye = [face_landmarks[i] for i in range(362, 463)]  # Landmark c·ªßa m·∫Øt ph·∫£i (index c√≥ th·ªÉ thay ƒë·ªïi)
        
        # Ki·ªÉm tra xem c·∫£ hai m·∫Øt c√≥ ƒë∆∞·ª£c ph√°t hi·ªán kh√¥ng
        if left_eye and right_eye:
            img_h, img_w, _ = image.shape
            # T√≠nh c√°c ƒëi·ªÉm bounding box
            x_coords = [int(landmark.x * img_w) for landmark in face_landmarks]
            y_coords = [int(landmark.y * img_h) for landmark in face_landmarks]
            x1, x2 = min(x_coords), max(x_coords)
            y1, y2 = min(y_coords), max(y_coords)
            bbox_width, bbox_height = x2 - x1, y2 - y1
            if min(bbox_width, bbox_height) < 70:
                return None, None
            
            bbox = np.array([(x1, y1), (x2, y1), (x2, y2), (x1, y2)], dtype=np.int32)
            x1, y1, x2, y2 = max(0, x1), max(0, y1), min(img_w, x2), min(img_h, y2)
            return image[y1:y2, x1:x2], bbox

    return None, None

def train(saved_face, employee_id):
    """
    Hu·∫•n luy·ªán model v·ªõi t·∫≠p ·∫£nh.
    - Tham s·ªë: danh s√°ch ·∫£nh v√† m√£ nh√¢n vi√™n
    - K·∫øt qu·∫£: tr·∫£ v·ªÅ hu·∫•n luy·ªán th√†nh c√¥ng
    """
    global cls_model, cls_path, rec_model, cls_ready
    cls_ready = True
    images = saved_face
        
    session.query(Embedding).filter_by(employee_id=employee_id).delete()
    session.commit()
    embeddings, labels = [], []

    # üîπ Load t·∫•t c·∫£ embeddings c≈© t·ª´ database tr∆∞·ªõc khi train
    data = session.query(Embedding).all()
    for item in data:
        embeddings.append(np.frombuffer(item.embedding, dtype=np.float32).tolist())
        labels.append(item.employee_id)
    
    # üîπ Th√™m embeddings m·ªõi v√†o
    count = 0
    instances = []
    for image in images:
        face = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        face = cv2.resize(face, (160, 160))
        augmented_faces = augmentate(face)
        all_images = [face] + augmented_faces

        for img in all_images:
            count += 1
            img = img.astype('float32') / 255.0
            img = np.transpose(img, (2, 0, 1))
            img = np.expand_dims(img, axis=0)
            output = rec_model.run(None, {rec_model.get_inputs()[0].name: img})
            embedding = output[0][0].tobytes()

            instance = Embedding(embedding=embedding, employee_id=employee_id)
            instances.append(instance)

            embeddings.append(np.frombuffer(embedding, dtype=np.float32).tolist())
            labels.append(employee_id)
    
    session.add_all(instances)
    session.commit()
    print('S·ªë l∆∞·ª£ng ·∫£nh: ',count)
    previous_labels = len(set(labels))
    if previous_labels < 2:
        dummy_embedding = np.random.rand(len(embeddings[0])).tolist()
        embeddings.append(dummy_embedding)
        labels.append("unknown")
    if previous_labels < 3:
        dummy_embedding2 = np.random.rand(len(embeddings[0])).tolist()
        embeddings.append(dummy_embedding2)
        labels.append("unknown2")
    cls_model = LinearSVC(random_state=42)
    cls_model.fit(embeddings, labels)
    joblib.dump(cls_model, cls_path)
    del images, embeddings, labels
    return {'status': 'Th√†nh c√¥ng', 'classes': cls_model.classes_.tolist()}

def softmax(x, threshold, labels=None):
    e_x = np.exp(x - np.max(x))  # Tr√°nh overflow b·∫±ng c√°ch tr·ª´ ƒëi gi√° tr·ªã max
    probabilities = e_x / np.sum(e_x, axis=-1, keepdims=True)  # Gi·ªØ shape chu·∫©n
    result = labels[np.where(probabilities[0] > threshold)]
    if result.size > 0 and not np.isin("unknown", result) and not np.isin("unknown2", result):
        return result[np.argmax(probabilities[0] > threshold)]  # Tr·∫£ v·ªÅ gi√° tr·ªã c√≥ x√°c su·∫•t cao nh·∫•t
    else:
        return ""

def predict(face, threshold=0.6):
    """
    D·ª± ƒëo√°n danh t√≠nh khu√¥n m·∫∑t t·ª´ ·∫£nh.
    - ƒê·∫ßu v√†o: ·∫£nh khu√¥n m·∫∑t ƒë√£ c·∫Øt
    - Tr·∫£ v·ªÅ: employee_id ƒë√°p ·ª©ng ng∆∞·ª°ng
    """
    global cls_model, rec_model, cls_ready
    if not cls_ready: return ''

    # Ti·ªÅn x·ª≠ l√≠ ·∫£nh cho m√¥ h√¨nh Facenet
    img = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (160, 160))
    img = img.astype('float32') / 255.0
    img = np.transpose(img, (2, 0, 1))
    img = np.expand_dims(img, axis=0)
    outputs = rec_model.run(None, {rec_model.get_inputs()[0].name: img})
    x = cls_model.decision_function(np.array(outputs).reshape(1, -1))
    # D·ª± ƒëo√°n b·∫±ng SVC, t√≠nh x√°c su·∫•t, ki·ªÉm tra ng∆∞·ª°ng r·ªìi d·ª± ƒëo√°n
    predicted_label = softmax(x, threshold=threshold, labels=cls_model.classes_)
    del outputs, img
    return predicted_label