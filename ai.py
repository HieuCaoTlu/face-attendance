import os
import cv2
import joblib
import numpy as np
import mediapipe as mp
import onnxruntime as ort
from sklearn.svm import SVC
from database import session, Embedding

model_dir = os.path.join(os.path.dirname(__file__), 'models')
rec_path = os.path.join(model_dir, 'quantized_model.onnx')
cls_path = os.path.join(model_dir, 'face_classifier.pkl')
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    static_image_mode=False,  # False Ä‘á»ƒ sá»­ dá»¥ng video (dynamic)
    max_num_faces=1,  # Sá»‘ lÆ°á»£ng khuÃ´n máº·t tá»‘i Ä‘a cÃ³ thá»ƒ phÃ¡t hiá»‡n trong má»—i khung hÃ¬nh
    refine_landmarks=True,  # Lá»c landmarks (Ä‘iá»ƒm má»‘c) Ä‘á»ƒ cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n
    min_detection_confidence=0.5,  # Má»©c Ä‘á»™ tin cáº­y tá»‘i thiá»ƒu Ä‘á»ƒ phÃ¡t hiá»‡n khuÃ´n máº·t
)

def load_model():
    #Kiá»ƒm tra sá»± tá»“n táº¡i cá»§a cÃ¡c thÆ° má»¥c cáº§n thiáº¿t
    if not os.path.exists('models'):
        os.makedirs('models')

    #Kiá»ƒm tra File ONNX
    rec_model = ort.InferenceSession(rec_path)
    print("Model ONNX sáºµn sÃ ng")

    #Kiá»ƒm tra File SVC
    if not os.path.exists(cls_path):
        print("File SVC chÆ°a tá»“n táº¡i, Ä‘ang tÃ¡i táº¡o...")
        cls_model = SVC(kernel='linear', probability=True)
        joblib.dump(cls_model, cls_path)
    cls_model = joblib.load(cls_path)
    print("Model SVC sáºµn sÃ ng")

    return rec_model, cls_model

rec_model, cls_model = load_model()

def detect_face(image):
    """
    PhÃ¡t hiá»‡n khuÃ´n máº·t trong áº£nh vÃ  trÃ­ch xuáº¥t vÃ¹ng máº·t.
    - Tham sá»‘: áº£nh gá»‘c
    - Tráº£ vá»: áº£nh khuÃ´n máº·t Ä‘Ã£ cáº¯t vÃ  tá»a Ä‘á»™ bounding box
    """
    global mp_face_mesh, face_mesh
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = face_mesh.process(image_rgb)
    if results.multi_face_landmarks and len(results.multi_face_landmarks) > 0:
        # Khi phÃ¡t hiá»‡n Ä‘Æ°á»£c máº·t, láº¥y tá»a Ä‘á»™, chuáº©n hÃ³a tá»a Ä‘á»™
        face_landmarks = results.multi_face_landmarks[0].landmark
        img_h, img_w, _ = image.shape
        x_coords = [int(landmark.x * img_w) for landmark in face_landmarks]
        y_coords = [int(landmark.y * img_h) for landmark in face_landmarks]

        # XÃ¡c Ä‘á»‹nh giá»›i háº¡n bounding box
        x1, x2 = min(x_coords), max(x_coords)
        y1, y2 = min(y_coords), max(y_coords)
        bbox = np.array([(x1, y1), (x2, y1), (x2, y2), (x1, y2)], dtype=np.int32)
        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(img_w, x2), min(img_h, y2)
        return image[y1:y2, x1:x2], bbox
    return None, None

def train(saved_face, employee_id):
    """
    Huáº¥n luyá»‡n model vá»›i táº­p áº£nh.
    - Tham sá»‘: danh sÃ¡ch áº£nh vÃ  mÃ£ nhÃ¢n viÃªn
    - Káº¿t quáº£: tráº£ vá» huáº¥n luyá»‡n thÃ nh cÃ´ng
    """
    global cls_model, cls_path, rec_model
    images = []
    for buffer in saved_face:
        image_array = np.frombuffer(buffer, np.uint8)
        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
        images.append(image)
        
    session.query(Embedding).filter_by(employee_id=employee_id).delete()
    session.commit()
    embeddings, labels = [], []
    # ðŸ”¹ Load táº¥t cáº£ embeddings cÅ© tá»« database trÆ°á»›c khi train
    data = session.query(Embedding).all()
    for item in data:
        embeddings.append(np.frombuffer(item.embedding, dtype=np.float32).tolist())
        labels.append(item.employee_id)

    # ðŸ”¹ ThÃªm embeddings má»›i vÃ o
    for image in images:
        face, bbox = detect_face(image)
        if face is None: continue
        img = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (160, 160))
        img = img.astype('float32') / 255.0
        img = np.transpose(img, (2, 0, 1))
        img = np.expand_dims(img, axis=0)
        output = rec_model.run(None, {rec_model.get_inputs()[0].name: img})
        embedding = output[0][0].tobytes()

        # LÆ°u vÃ o database
        instance = Embedding(embedding=embedding, employee_id=employee_id)
        session.add(instance)
        session.commit()

        embeddings.append(np.frombuffer(embedding, dtype=np.float32).tolist())
        labels.append(employee_id)

    if len(set(labels)) == 1:
        print("Chá»‰ cÃ³ má»™t nhÃ£n duy nháº¥t, thÃªm embedding giáº£...")
        dummy_embedding = np.random.rand(len(embeddings[0])).tolist()
        embeddings.append(dummy_embedding)
        labels.append("unknown")

    cls_model = SVC(kernel='linear', probability=True)
    cls_model.fit(embeddings, labels)
    joblib.dump(cls_model, cls_path)
    return {'status': 'ThÃ nh cÃ´ng', 'classes': cls_model.classes_.tolist()}

def predict(face, threshold=0.8):
    """
    Dá»± Ä‘oÃ¡n danh tÃ­nh khuÃ´n máº·t tá»« áº£nh.
    - Äáº§u vÃ o: áº£nh khuÃ´n máº·t Ä‘Ã£ cáº¯t
    - Tráº£ vá»: employee_id Ä‘Ã¡p á»©ng ngÆ°á»¡ng
    """
    global cls_model, rec_model

    # Tiá»n xá»­ lÃ­ áº£nh cho mÃ´ hÃ¬nh Facenet
    img = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (160, 160))
    img = img.astype('float32') / 255.0
    img = np.transpose(img, (2, 0, 1))
    img = np.expand_dims(img, axis=0)
    outputs = rec_model.run(None, {rec_model.get_inputs()[0].name: img})

    # Dá»± Ä‘oÃ¡n báº±ng SVC, tÃ­nh xÃ¡c suáº¥t, kiá»ƒm tra ngÆ°á»¡ng rá»“i dá»± Ä‘oÃ¡n
    predicted_probs = cls_model.predict_proba(outputs[0])
    confidence, predicted_index = np.max(predicted_probs), np.argmax(predicted_probs, axis=1)[0]
    predicted_label = cls_model.classes_[predicted_index]
    return predicted_label if confidence >= threshold and predicted_label.lower() != "unknown" else ""